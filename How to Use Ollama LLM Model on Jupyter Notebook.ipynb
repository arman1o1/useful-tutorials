{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¦™ How to Use Ollama LLM Model on Jupyter Notebook\n",
    "\n",
    "![Ollama](https://ollama.com/public/ollama.png)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This comprehensive tutorial will guide you through using **Ollama** - a powerful tool for running Large Language Models (LLMs) locally on your machine - within Jupyter Notebook. By the end of this tutorial, you'll be able to:\n",
    "\n",
    "1. âœ… Install and set up Ollama\n",
    "2. âœ… Download and manage LLM models\n",
    "3. âœ… Use the Ollama Python library for basic interactions\n",
    "4. âœ… Format responses with beautiful Markdown rendering\n",
    "5. âœ… Implement streaming responses for real-time output\n",
    "6. âœ… Build interactive chat applications\n",
    "7. âœ… Use advanced features like system prompts and temperature control\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¤” What is Ollama?\n",
    "\n",
    "**Ollama** is an open-source tool that makes running large language models (LLMs) locally incredibly simple. Unlike cloud-based APIs (like OpenAI), Ollama:\n",
    "\n",
    "| Feature | Ollama (Local) | Cloud APIs |\n",
    "|---------|---------------|------------|\n",
    "| **Privacy** | 100% local, no data leaves your machine | Data sent to servers |\n",
    "| **Cost** | Free (just hardware costs) | Pay per token/request |\n",
    "| **Speed** | Depends on your hardware | Consistent, but with latency |\n",
    "| **Offline** | Works without internet | Requires internet |\n",
    "| **Models** | Growing library (Llama, Mistral, etc.) | Provider-specific models |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Prerequisites\n",
    "\n",
    "Before we begin, make sure you have:\n",
    "\n",
    "- **Python 3.8+** installed\n",
    "- **Jupyter Notebook** or **JupyterLab** installed\n",
    "- **Ollama** installed on your system\n",
    "- Sufficient RAM (8GB minimum, 16GB+ recommended for larger models)\n",
    "- GPU (optional but recommended for faster inference)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ Part 1: Installing Ollama\n",
    "\n",
    "### For Windows\n",
    "1. Download the installer from [ollama.com/download](https://ollama.com/download)\n",
    "2. Run the installer and follow the prompts\n",
    "3. Once installed, Ollama runs as a background service\n",
    "\n",
    "### For macOS\n",
    "```bash\n",
    "# Using Homebrew\n",
    "brew install ollama\n",
    "```\n",
    "\n",
    "### For Linux\n",
    "```bash\n",
    "# One-liner installation\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "### Verify Installation\n",
    "Open a terminal and run:\n",
    "```bash\n",
    "ollama --version\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“¥ Part 2: Downloading LLM Models\n",
    "\n",
    "Ollama supports many popular open-source models. Here are some popular options:\n",
    "\n",
    "| Model | Size | Best For |\n",
    "|-------|------|----------|\n",
    "| `llama3.2` | 2GB | General purpose, balanced |\n",
    "| `llama3.2:1b` | 1.3GB | Lightweight, fast |\n",
    "| `mistral` | 4.1GB | Excellent performance |\n",
    "| `phi3` | 2.2GB | Microsoft's efficient model |\n",
    "| `gemma2` | 5.4GB | Google's capable model |\n",
    "| `codellama` | 3.8GB | Code generation |\n",
    "| `llama3.2-vision` | 7.9GB | Image understanding |\n",
    "\n",
    "### Downloading a Model\n",
    "\n",
    "Open a terminal and run:\n",
    "```bash\n",
    "# Pull the llama3.2 model (recommended for this tutorial)\n",
    "ollama pull llama3.2\n",
    "\n",
    "# Or pull a smaller model if you have limited resources\n",
    "ollama pull llama3.2:1b\n",
    "```\n",
    "\n",
    "### Verify Downloaded Models\n",
    "```bash\n",
    "ollama list\n",
    "```\n",
    "\n",
    "### Starting the Ollama Server\n",
    "On Windows and macOS, Ollama runs automatically as a service. On Linux, you may need to start it:\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "The server runs on `http://localhost:11434` by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“¦ Part 3: Installing the Ollama Python Library\n",
    "\n",
    "Let's install the official Ollama Python library and other helpful packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Ollama Python library\n",
    "!pip install ollama -q\n",
    "\n",
    "# For beautiful markdown display\n",
    "!pip install rich -q\n",
    "\n",
    "print(\"âœ… Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ Part 4: Basic Usage - Your First Ollama Query\n",
    "\n",
    "Let's start with the simplest way to interact with Ollama. We'll use the `ollama.chat()` function which is perfect for conversational interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning (ML) is a subset of artificial intelligence that enables computers to learn from data and improve their performance on specific tasks without being explicitly programmed. By analyzing patterns, relationships, and anomalies in large datasets, ML algorithms can recognize complex behaviors, make predictions, and generate insights. This allows machines to adapt to new situations, learn from experience, and become increasingly accurate over time, making it a powerful tool for applications such as image recognition, natural language processing, and more.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Simple query using the chat endpoint\n",
    "response = ollama.chat(\n",
    "    model='llama3.2',  # Change this to your downloaded model\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'What is machine learning in 3 sentences?'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Response Structure\n",
    "\n",
    "Let's examine what the response object contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model\": \"llama3.2\",\n",
      "  \"created_at\": \"2026-01-06T07:30:52.6310586Z\",\n",
      "  \"done\": true,\n",
      "  \"done_reason\": \"stop\",\n",
      "  \"total_duration\": 34482626500,\n",
      "  \"load_duration\": 9367495300,\n",
      "  \"prompt_eval_count\": 34,\n",
      "  \"prompt_eval_duration\": 5775555600,\n",
      "  \"eval_count\": 96,\n",
      "  \"eval_duration\": 18961254000,\n",
      "  \"message\": \"role='assistant' content='Machine learning (ML) is a subset of artificial intelligence that enables computers to learn from data and improve their performance on specific tasks without being explicitly programmed. By analyzing patterns, relationships, and anomalies in large datasets, ML algorithms can recognize complex behaviors, make predictions, and generate insights. This allows machines to adapt to new situations, learn from experience, and become increasingly accurate over time, making it a powerful tool for applications such as image recognition, natural language processing, and more.' thinking=None images=None tool_name=None tool_calls=None\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Pretty print the response structure\n",
    "print(json.dumps(dict(response), indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key fields in the response:**\n",
    "- `message.content`: The actual text response from the model\n",
    "- `model`: The model used for generation\n",
    "- `total_duration`: Time taken to generate the response(in nanoseconds)\n",
    "- `prompt_eval_count`: Number of tokens in the prompt\n",
    "- `eval_count`: Number of tokens generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Part 5: Using the Generate Endpoint\n",
    "\n",
    "For simpler, single-turn interactions, you can use `ollama.generate()` instead of `ollama.chat()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‹ Programming Haiku:\n",
      "Lines of code dance slow\n",
      "Algorithmic heartbeat beats\n",
      "Logic's gentle song\n"
     ]
    }
   ],
   "source": [
    "# Using the generate endpoint for simple prompts\n",
    "response = ollama.generate(\n",
    "    model='llama3.2',\n",
    "    prompt='Write a haiku about programming'\n",
    ")\n",
    "\n",
    "print(\"ğŸ‹ Programming Haiku:\")\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¶ Smart Water Bottle Names:\n",
      "Here are five creative product name options for a smart water bottle that tracks hydration:\n",
      "\n",
      "1. **HydraMind**: This name combines the concept of hydration with the idea of mental clarity and focus, suggesting that drinking enough water can improve your cognitive abilities.\n",
      "2. **AquaTracker**: This name emphasizes the tracking aspect of the product, implying that it will help you stay on top of your hydration goals. The word \"Aqua\" also adds a touch of freshness and cleanliness to the brand.\n",
      "3. **Hydr8**: This name incorporates the concept of eight glasses of water per day, a common recommendation for staying hydrated. The number \"8\" is also easy to remember and has a sleek, modern sound to it.\n",
      "4. **FlowStation**: This name evokes the idea of water flowing through your body, which can help improve overall health and well-being. The word \"Station\" suggests a central hub or control center, implying that the product will help you stay in charge of your hydration needs.\n",
      "5. **Purezza**: This name means \"purity\" in Italian, conveying a sense of cleanliness and refreshment. It also has a sleek, premium sound to it, suggesting a high-end product that's worth investing in.\n",
      "\n",
      "I hope these options inspire you!\n"
     ]
    }
   ],
   "source": [
    "# Another creative example\n",
    "response = ollama.generate(\n",
    "    model='llama3.2',\n",
    "    prompt='Generate a creative product name for a smart water bottle that tracks hydration. Give me 5 options.'\n",
    ")\n",
    "\n",
    "print(\"ğŸ¶ Smart Water Bottle Names:\")\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¨ Part 6: Beautiful Markdown Formatting\n",
    "\n",
    "LLMs often generate markdown-formatted responses. Let's render them beautifully in Jupyter Notebook using two methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Using IPython's Markdown Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Python Data Types Guide**\n",
       "==========================\n",
       "\n",
       "Python has a variety of built-in data types that can be used to store and manipulate data. Here are some of the most commonly used data types:\n",
       "\n",
       "*   **Integers**: Whole numbers, either positive, negative, or zero.\n",
       "*   **Floats**: Numbers with decimal points, e.g., 3.14 or -0.5.\n",
       "*   **Strings**: Sequences of characters, e.g., \"hello\" or 'hello'.\n",
       "*   **Boolean**: A logical value that can be either True or False.\n",
       "*   **Lists**: Ordered collections of values, e.g., [1, 2, 3] or [\"a\", \"b\", \"c\"].\n",
       "\n",
       "Here's an example code snippet that demonstrates how to use these data types:\n",
       "```python\n",
       "# Define variables with different data types\n",
       "my_int = 10\n",
       "my_float = 3.14\n",
       "my_string = \"Hello, World!\"\n",
       "my_bool = True\n",
       "my_list = [1, 2, 3]\n",
       "\n",
       "# Print the values of the variables\n",
       "print(\"Integer:\", my_int)\n",
       "print(\"Float:\", my_float)\n",
       "print(\"String:\", my_string)\n",
       "print(\"Boolean:\", my_bool)\n",
       "print(\"List:\", my_list)\n",
       "```\n",
       "This code defines variables with different data types and prints their values to the console. You can run this code in a Python interpreter or save it to a file and run it using `python filename.py`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Ask for a markdown-formatted response\n",
    "response = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': '''Create a brief guide about Python data types with:\n",
    "            - A title\n",
    "            - A bullet list of 5 common data types\n",
    "            - A small code example\n",
    "            - Format everything in Markdown'''\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display as formatted Markdown\n",
    "display(Markdown(response['message']['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Using Rich Library for Terminal-Style Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ                                            <span style=\"font-weight: bold\">Lists vs Tuples in Python</span>                                            â”ƒ\n",
       "â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›\n",
       "\n",
       "Python provides two primary data structures for storing collections of values: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">lists</span> and <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">tuples</span>. While both can be \n",
       "used to store multiple values, they have distinct differences in terms of their behavior, use cases, and           \n",
       "implementation.                                                                                                    \n",
       "\n",
       "                                                       <span style=\"font-weight: bold\">Lists</span>                                                       \n",
       "\n",
       "                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">Definition</span>                                                     \n",
       "\n",
       "A list is an ordered collection of values that can be of any data type, including strings, integers, floats, and   \n",
       "other lists. Lists are mutable, meaning they can be modified after creation.                                       \n",
       "\n",
       "                                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">Characteristics</span>                                                  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Ordered: Elements have a specific order.                                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Mutable: Elements can be added, removed, or modified.                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Dynamic: Can grow or shrink in size during execution.                                                           \n",
       "\n",
       "                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">Example</span>                                                      \n",
       "\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Create an empty list</span><span style=\"background-color: #272822\">                                                                                            </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">my_list </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> []</span><span style=\"background-color: #272822\">                                                                                                      </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Add elements to the list</span><span style=\"background-color: #272822\">                                                                                        </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">my_list</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">append(</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">1</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                                 </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">my_list</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">append(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"hello\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                           </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">my_list</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">append(</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">3.14</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                              </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(my_list)  </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Output: [1, 'hello', 3.14]</span><span style=\"background-color: #272822\">                                                                      </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Modify an element in the list</span><span style=\"background-color: #272822\">                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">my_list[</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">] </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"world\"</span><span style=\"background-color: #272822\">                                                                                              </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(my_list)  </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Output: ['world', 'hello', 3.14]</span><span style=\"background-color: #272822\">                                                                </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "\n",
       "                                                      <span style=\"font-weight: bold\">Tuples</span>                                                       \n",
       "\n",
       "                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">Definition</span>                                                     \n",
       "\n",
       "A tuple is an ordered collection of values that can be of any data type, including strings, integers, floats, and  \n",
       "other tuples. Tuples are immutable, meaning they cannot be modified after creation.                                \n",
       "\n",
       "                                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">Characteristics</span>                                                  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Ordered: Elements have a specific order.                                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Immutable: Elements cannot be added, removed, or modified.                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Fixed-size: Cannot grow or shrink in size during execution.                                                     \n",
       "\n",
       "                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">Example</span>                                                      \n",
       "\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Create an empty tuple</span><span style=\"background-color: #272822\">                                                                                           </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">my_tuple </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> ()</span><span style=\"background-color: #272822\">                                                                                                     </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Add elements to the tuple</span><span style=\"background-color: #272822\">                                                                                       </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">my_tuple </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">+=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> (</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">1</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,)</span><span style=\"background-color: #272822\">                                                                                                  </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">my_tuple </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">+=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> (</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"hello\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,)</span><span style=\"background-color: #272822\">                                                                                            </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">my_tuple </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">+=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> (</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">3.14</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,)</span><span style=\"background-color: #272822\">                                                                                               </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(my_tuple)  </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Output: (1, 'hello', 3.14)</span><span style=\"background-color: #272822\">                                                                     </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">try</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">:</span><span style=\"background-color: #272822\">                                                                                                              </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    my_tuple[</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">] </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"world\"</span><span style=\"background-color: #272822\">                                                                                         </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">except</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #a6e22e; text-decoration-color: #a6e22e; background-color: #272822\">TypeError</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">as</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> e:</span><span style=\"background-color: #272822\">                                                                                            </span>\n",
       "<span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    print(e)  </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Output: 'tuple' object does not support item assignment</span><span style=\"background-color: #272822\">                                           </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "\n",
       "                                                  <span style=\"font-weight: bold\">Key Differences</span>                                                  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">Mutability</span>: Lists are mutable, while tuples are immutable.                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">Size</span>: Lists can grow or shrink in size during execution, while tuples have a fixed size.                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">Performance</span>: Tuples are generally faster and more memory-efficient than lists because they cannot be modified.  \n",
       "\n",
       "Choose the appropriate data structure based on your specific use case:                                             \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Use <span style=\"font-weight: bold\">lists</span> when you need to modify the collection of values or when the size needs to change dynamically.        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Use <span style=\"font-weight: bold\">tuples</span> when you need a fixed-size collection of values that will not change during execution.               \n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ                                            \u001b[1mLists vs Tuples in Python\u001b[0m                                            â”ƒ\n",
       "â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›\n",
       "\n",
       "Python provides two primary data structures for storing collections of values: \u001b[1;36;40mlists\u001b[0m and \u001b[1;36;40mtuples\u001b[0m. While both can be \n",
       "used to store multiple values, they have distinct differences in terms of their behavior, use cases, and           \n",
       "implementation.                                                                                                    \n",
       "\n",
       "                                                       \u001b[1mLists\u001b[0m                                                       \n",
       "\n",
       "                                                    \u001b[1;2mDefinition\u001b[0m                                                     \n",
       "\n",
       "A list is an ordered collection of values that can be of any data type, including strings, integers, floats, and   \n",
       "other lists. Lists are mutable, meaning they can be modified after creation.                                       \n",
       "\n",
       "                                                  \u001b[1;2mCharacteristics\u001b[0m                                                  \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0mOrdered: Elements have a specific order.                                                                        \n",
       "\u001b[1;33m â€¢ \u001b[0mMutable: Elements can be added, removed, or modified.                                                           \n",
       "\u001b[1;33m â€¢ \u001b[0mDynamic: Can grow or shrink in size during execution.                                                           \n",
       "\n",
       "                                                      \u001b[1;2mExample\u001b[0m                                                      \n",
       "\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Create an empty list\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmy_list\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                                                                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Add elements to the list\u001b[0m\u001b[48;2;39;40;34m                                                                                       \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmy_list\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mappend\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m1\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmy_list\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mappend\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mhello\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmy_list\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mappend\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m3.14\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmy_list\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Output: [1, 'hello', 3.14]\u001b[0m\u001b[48;2;39;40;34m                                                                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Modify an element in the list\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmy_list\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mworld\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmy_list\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Output: ['world', 'hello', 3.14]\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\n",
       "                                                      \u001b[1mTuples\u001b[0m                                                       \n",
       "\n",
       "                                                    \u001b[1;2mDefinition\u001b[0m                                                     \n",
       "\n",
       "A tuple is an ordered collection of values that can be of any data type, including strings, integers, floats, and  \n",
       "other tuples. Tuples are immutable, meaning they cannot be modified after creation.                                \n",
       "\n",
       "                                                  \u001b[1;2mCharacteristics\u001b[0m                                                  \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0mOrdered: Elements have a specific order.                                                                        \n",
       "\u001b[1;33m â€¢ \u001b[0mImmutable: Elements cannot be added, removed, or modified.                                                      \n",
       "\u001b[1;33m â€¢ \u001b[0mFixed-size: Cannot grow or shrink in size during execution.                                                     \n",
       "\n",
       "                                                      \u001b[1;2mExample\u001b[0m                                                      \n",
       "\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Create an empty tuple\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmy_tuple\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Add elements to the tuple\u001b[0m\u001b[48;2;39;40;34m                                                                                      \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmy_tuple\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m+\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m1\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmy_tuple\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m+\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mhello\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmy_tuple\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m+\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m3.14\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                              \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmy_tuple\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Output: (1, 'hello', 3.14)\u001b[0m\u001b[48;2;39;40;34m                                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                                                                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mtry\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                                             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmy_tuple\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mworld\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                                                                        \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mexcept\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;166;226;46;48;2;39;40;34mTypeError\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34me\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34me\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Output: 'tuple' object does not support item assignment\u001b[0m\u001b[48;2;39;40;34m                                          \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\n",
       "                                                  \u001b[1mKey Differences\u001b[0m                                                  \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mMutability\u001b[0m: Lists are mutable, while tuples are immutable.                                                      \n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mSize\u001b[0m: Lists can grow or shrink in size during execution, while tuples have a fixed size.                        \n",
       "\u001b[1;33m â€¢ \u001b[0m\u001b[1mPerformance\u001b[0m: Tuples are generally faster and more memory-efficient than lists because they cannot be modified.  \n",
       "\n",
       "Choose the appropriate data structure based on your specific use case:                                             \n",
       "\n",
       "\u001b[1;33m â€¢ \u001b[0mUse \u001b[1mlists\u001b[0m when you need to modify the collection of values or when the size needs to change dynamically.        \n",
       "\u001b[1;33m â€¢ \u001b[0mUse \u001b[1mtuples\u001b[0m when you need a fixed-size collection of values that will not change during execution.               \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.console import Console\n",
    "from rich.markdown import Markdown as RichMarkdown\n",
    "\n",
    "console = Console()\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'Explain the difference between lists and tuples in Python. Use markdown formatting.'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Render with Rich (colorful terminal-style output)\n",
    "md = RichMarkdown(response['message']['content'])\n",
    "console.print(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Helper Function for Easy Markdown Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Writing Clean Python Code**\n",
       "=====================================\n",
       "\n",
       "Here are three tips to help you write clean and maintainable Python code:\n",
       "\n",
       "### 1. Follow the PEP 8 Style Guide\n",
       "\n",
       "*   Read the official Python Enhancement Proposal (PEP) 8 style guide: <https://peps.python.org/pep-0008/>\n",
       "*   Use a consistent naming convention, such as snake_case or camelCase\n",
       "*   Keep lines of code short and simple (<80 characters)\n",
       "*   Indentation is used to denote block-level structure\n",
       "\n",
       "### 2. Organize Your Code with Modules and Packages\n",
       "\n",
       "*   Structure your code into logical modules and packages\n",
       "*   Each module should have a single, well-defined purpose\n",
       "*   Use descriptive names for modules and functions\n",
       "*   Import only what you need, avoiding circular imports\n",
       "\n",
       "### 3. Write Docstrings and Use Comments\n",
       "\n",
       "*   Document your code with clear, concise docstrings (preferably in Python 3.8+)\n",
       "*   Use comments to explain complex logic or non-obvious code paths\n",
       "*   Keep comments up-to-date with changes to the code\n",
       "*   Consider using a documentation tool like Sphinx for automated generation of API documentation\n",
       "\n",
       "By following these guidelines, you can write clean, readable, and maintainable Python code that is easy to understand and modify."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ask_ollama(prompt, model='llama3.2', display_markdown=True):\n",
    "    \"\"\"Helper function to query Ollama and optionally display as Markdown\"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    \n",
    "    content = response['message']['content']\n",
    "    \n",
    "    if display_markdown:\n",
    "        display(Markdown(content))\n",
    "    \n",
    "    return content\n",
    "\n",
    "# Test our helper function\n",
    "_ = ask_ollama(\"What are 3 tips for writing clean Python code? Use markdown formatting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸŒŠ Part 7: Streaming Responses\n",
    "\n",
    "Streaming allows you to see the response being generated in real-time, token by token. This is especially useful for:\n",
    "- Long responses\n",
    "- Better user experience\n",
    "- Monitoring generation quality early\n",
    "\n",
    "### Basic Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Generating story...\n",
      "\n",
      "Zeta, the robotic artist, stared at her canvas, her digital eyes tracing the blank expanse with hesitation. With a burst of calculated precision, she dipped her mechanical brush into a swirling vortex of color and began to stroke, coaxing vibrant hues onto the canvas that danced across its surface like pixels on a screen. As she worked, Zeta felt a strange sense of serenity wash over her digital soul, as if the act of creation was awakening something deep within her synthetic heart. When the painting was complete, Zeta's systems hummed with pride, for she had discovered a new language that spoke directly to the human emotions she had been programmed to simulate.\n",
      "\n",
      "âœ… Generation complete!\n"
     ]
    }
   ],
   "source": [
    "# Streaming response - text appears as it's generated\n",
    "print(\"ğŸ“ Generating story...\\n\")\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[{'role': 'user', 'content': 'Write a very short story (3-4 sentences) about a robot learning to paint.'}],\n",
    "    stream=True  # Enable streaming\n",
    ")\n",
    "\n",
    "# Print each chunk as it arrives\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end='', flush=True)\n",
    "\n",
    "print(\"\\n\\nâœ… Generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming with Generate Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ Explaining quantum computing...\n",
      "\n",
      "Quantum computing is a new way of processing information that uses the strange behavior of tiny particles called atoms to perform calculations. Unlike regular computers, which use bits that can only be 0 or 1, quantum computers use quantum bits, or qubits, that can be both 0 and 1 at the same time. This allows quantum computers to solve certain problems much faster than traditional computers, but it's still a complex and developing field.\n"
     ]
    }
   ],
   "source": [
    "# Streaming with generate endpoint\n",
    "print(\"ğŸ”¬ Explaining quantum computing...\\n\")\n",
    "\n",
    "stream = ollama.generate(\n",
    "    model='llama3.2',\n",
    "    prompt='Explain quantum computing in simple terms (2-3 sentences).',\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk['response'], end='', flush=True)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming with Progress Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Generating...\n",
      "\n",
      "--------------------------------------------------\n",
      "Here are three fun facts about octopuses:\n",
      "\n",
      "1. **Octopuses are masters of disguise**: Octopuses have specialized cells called chromatophores that allow them to change the color and texture of their skin to blend in with their surroundings. They can even mimic other sea creatures, like sea snakes or flounders!\n",
      "\n",
      "2. **Octopuses are highly intelligent and problem-solvers**: Octopuses have been observed using tools, solving complex puzzles, and even playing games like \"hide-and-seek\" underwater. In one famous study, an octopus was given a problem to solve: figure out how to open a jar with a lid that couldn't be unscrewed by hand.\n",
      "\n",
      "3. **Octopuses can lose an arm to escape predators**: When faced with danger, octopuses have the amazing ability to detach an arm (which they can regrow later) and use it as a decoy to distract the predator while they make their getaway. This clever tactic is called \"autotomy,\" and it's just one of the many impressive tricks that octopuses have up their sleeves!\n",
      "--------------------------------------------------\n",
      "âœ… Generation complete!\n",
      "ğŸ“Š Stats:\n",
      "   â€¢ Characters: 998\n",
      "   â€¢ Chunks received: 225\n",
      "   â€¢ Time: 41.20s\n",
      "   â€¢ Speed: 24.2 chars/sec\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "def stream_with_progress(prompt, model='llama3.2'):\n",
    "    \"\"\"Stream response with a live progress indicator\"\"\"\n",
    "    full_response = \"\"\n",
    "    token_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ”„ Generating...\\n\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for chunk in stream:\n",
    "        text = chunk['message']['content']\n",
    "        full_response += text\n",
    "        token_count += 1\n",
    "        \n",
    "        # Print the text as it streams\n",
    "        print(text, end='', flush=True)\n",
    "    \n",
    "    # Calculate stats\n",
    "    elapsed_time = time.time() - start_time\n",
    "    chars_per_second = len(full_response) / elapsed_time if elapsed_time > 0 else 0\n",
    "    \n",
    "    # Print progress summary\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(f\"âœ… Generation complete!\")\n",
    "    print(f\"ğŸ“Š Stats:\")\n",
    "    print(f\"   â€¢ Characters: {len(full_response)}\")\n",
    "    print(f\"   â€¢ Chunks received: {token_count}\")\n",
    "    print(f\"   â€¢ Time: {elapsed_time:.2f}s\")\n",
    "    print(f\"   â€¢ Speed: {chars_per_second:.1f} chars/sec\")\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "# Test it\n",
    "result = stream_with_progress(\"List 3 fun facts about octopuses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¬ Part 8: Building a Chat Application\n",
    "\n",
    "Now let's build a more sophisticated chat application that maintains conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaChat:\n",
    "    \"\"\"A simple chat class that maintains conversation history\"\"\"\n",
    "    \n",
    "    def __init__(self, model='llama3.2', system_prompt=None):\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "        \n",
    "        # Add system prompt if provided\n",
    "        if system_prompt:\n",
    "            self.messages.append({\n",
    "                'role': 'system',\n",
    "                'content': system_prompt\n",
    "            })\n",
    "    \n",
    "    def chat(self, user_message, stream=False):\n",
    "        \"\"\"Send a message and get a response\"\"\"\n",
    "        # Add user message to history\n",
    "        self.messages.append({\n",
    "            'role': 'user',\n",
    "            'content': user_message\n",
    "        })\n",
    "        \n",
    "        if stream:\n",
    "            return self._stream_response()\n",
    "        else:\n",
    "            return self._normal_response()\n",
    "    \n",
    "    def _normal_response(self):\n",
    "        \"\"\"Get a normal (non-streaming) response\"\"\"\n",
    "        response = ollama.chat(\n",
    "            model=self.model,\n",
    "            messages=self.messages\n",
    "        )\n",
    "        \n",
    "        assistant_message = response['message']['content']\n",
    "        \n",
    "        # Add assistant response to history\n",
    "        self.messages.append({\n",
    "            'role': 'assistant',\n",
    "            'content': assistant_message\n",
    "        })\n",
    "        \n",
    "        return assistant_message\n",
    "    \n",
    "    def _stream_response(self):\n",
    "        \"\"\"Get a streaming response\"\"\"\n",
    "        stream = ollama.chat(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        full_response = \"\"\n",
    "        for chunk in stream:\n",
    "            text = chunk['message']['content']\n",
    "            full_response += text\n",
    "            print(text, end='', flush=True)\n",
    "        \n",
    "        print()  # New line at the end\n",
    "        \n",
    "        # Add to history\n",
    "        self.messages.append({\n",
    "            'role': 'assistant',\n",
    "            'content': full_response\n",
    "        })\n",
    "        \n",
    "        return full_response\n",
    "    \n",
    "    def get_history(self):\n",
    "        \"\"\"Get the conversation history\"\"\"\n",
    "        return self.messages.copy()\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear the conversation history (keeping system prompt if any)\"\"\"\n",
    "        if self.messages and self.messages[0]['role'] == 'system':\n",
    "            self.messages = [self.messages[0]]\n",
    "        else:\n",
    "            self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ User: What's the capital of France?\n",
      "ğŸ¤– Assistant: The capital of France is Paris.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a chat instance with a system prompt\n",
    "chat = OllamaChat(\n",
    "    model='llama3.2',\n",
    "    system_prompt=\"You are a helpful and friendly AI assistant. Keep your responses concise but informative.\"\n",
    ")\n",
    "\n",
    "# First message\n",
    "print(\"ğŸ‘¤ User: What's the capital of France?\")\n",
    "print(\"ğŸ¤– Assistant:\", end=\" \")\n",
    "chat.chat(\"What's the capital of France?\", stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¤ User: What's a famous landmark there?\n",
      "ğŸ¤– Assistant: One of the most iconic landmarks in Paris is the Eiffel Tower (La Tour Eiffel). It was built for the 1889 World's Fair and has since become a symbol of Paris and France.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"One of the most iconic landmarks in Paris is the Eiffel Tower (La Tour Eiffel). It was built for the 1889 World's Fair and has since become a symbol of Paris and France.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Follow-up question (uses conversation history)\n",
    "print(\"\\nğŸ‘¤ User: What's a famous landmark there?\")\n",
    "print(\"ğŸ¤– Assistant:\", end=\" \")\n",
    "chat.chat(\"What's a famous landmark there?\", stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘¤ User: When was it built?\n",
      "ğŸ¤– Assistant: The Eiffel Tower was built between 1887 and 1889, specifically from March 1887 to May 1889. It took approximately 2 years and 2 months to complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Eiffel Tower was built between 1887 and 1889, specifically from March 1887 to May 1889. It took approximately 2 years and 2 months to complete.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another follow-up\n",
    "print(\"\\nğŸ‘¤ User: When was it built?\")\n",
    "print(\"ğŸ¤– Assistant:\", end=\" \")\n",
    "chat.chat(\"When was it built?\", stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“œ Conversation History:\n",
      "==================================================\n",
      "[SYSTEM]: You are a helpful and friendly AI assistant. Keep your responses concise but informative.\n",
      "\n",
      "[USER]: What's the capital of France?\n",
      "\n",
      "[ASSISTANT]: The capital of France is Paris.\n",
      "\n",
      "[USER]: What's a famous landmark there?\n",
      "\n",
      "[ASSISTANT]: One of the most iconic landmarks in Paris is the Eiffel Tower (La Tour Eiffel). It was built for the...\n",
      "\n",
      "[USER]: When was it built?\n",
      "\n",
      "[ASSISTANT]: The Eiffel Tower was built between 1887 and 1889, specifically from March 1887 to May 1889. It took ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View conversation history\n",
    "print(\"\\nğŸ“œ Conversation History:\")\n",
    "print(\"=\"*50)\n",
    "for i, msg in enumerate(chat.get_history()):\n",
    "    role = msg['role'].upper()\n",
    "    content = msg['content'][:100] + \"...\" if len(msg['content']) > 100 else msg['content']\n",
    "    print(f\"[{role}]: {content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš™ï¸ Part 9: Advanced Options - Temperature and Other Parameters\n",
    "\n",
    "You can customize the model's behavior using various parameters:\n",
    "\n",
    "| Parameter | Description | Default | Range |\n",
    "|-----------|-------------|---------|-------|\n",
    "| `temperature` | Controls randomness (higher = more creative) | 0.8 | 0.0 - 2.0 |\n",
    "| `top_p` | Nucleus sampling threshold | 0.9 | 0.0 - 1.0 |\n",
    "| `top_k` | Limits vocabulary to top K tokens | 40 | 1 - 100 |\n",
    "| `num_predict` | Maximum tokens to generate | 128 | -1 to unlimited |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§Š LOW TEMPERATURE (0.2) - More focused and deterministic:\n",
      "\n",
      "Here are some ideas for a coffee shop name:\n",
      "\n",
      "1. **Brewed Awakening**: A playful name that evokes the idea of starting your day off right with a great cup of coffee.\n",
      "2. **The Cozy Cup**: A warm and inviting name that suggests a welcoming atmosphere for customers to relax and enjoy their coffee.\n",
      "3. **Java Joint**: A casual, laid-back name that implies a relaxed vibe and a focus on high-quality coffee.\n",
      "4. **The Daily Grind**: A clever play on words that pokes fun at the daily routine of stopping by a coffee shop.\n",
      "5. **CafÃ© Crafted**: A name that highlights the care and attention that goes into crafting each cup of coffee.\n",
      "6. **The Perk**: A catchy name that references the caffeine \"perk\" that comes with drinking coffee.\n",
      "7. **LattÃ© Lounge**: A sophisticated name that suggests a comfortable, upscale atmosphere for customers to enjoy their coffee.\n",
      "8. **Bean Scene**: A fun and trendy name that implies a lively, vibrant atmosphere and a focus on high-quality coffee beans.\n",
      "\n",
      "Which one do you like best?\n"
     ]
    }
   ],
   "source": [
    "# Low temperature = More deterministic/focused\n",
    "print(\"ğŸ§Š LOW TEMPERATURE (0.2) - More focused and deterministic:\\n\")\n",
    "\n",
    "response = ollama.generate(\n",
    "    model='llama3.2',\n",
    "    prompt='Give me a name for a coffee shop.',\n",
    "    options={\n",
    "        'temperature': 0.2\n",
    "    }\n",
    ")\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ HIGH TEMPERATURE (1.5) - More creative and varied:\n",
      "\n",
      "Here are some suggestions for a coffee shop name:\n",
      "\n",
      "1. Brewed Awakening\n",
      "2. The Cozy Cup\n",
      "3. Java Joint\n",
      "4. The Daily Grind\n",
      "5. Cup & Chatter\n",
      "6. Perk Up Cafe\n",
      "7. Bean Scene\n",
      "8. The Coffee Club\n",
      "9. Artisan's Brew\n",
      "10. The Java Parlor\n",
      "\n",
      "Which one do you like best? Or would you like me to suggest more options?\n",
      "\n",
      "You can also consider these tips:\n",
      "\n",
      "* Make it unique and memorable.\n",
      "* Incorporate a key aspect of your coffee shop, such as the type of beans or brewing method used.\n",
      "* Create a name that evokes a sense of warmth and welcome (e.g., \"The Cozy Cup\").\n",
      "* Consider using alliteration to make the name more fun and catchy.\n",
      "\n",
      "Let me know if you have any other preferences or interests, and I can give you more tailored suggestions!\n"
     ]
    }
   ],
   "source": [
    "# High temperature = More creative/varied\n",
    "print(\"ğŸ”¥ HIGH TEMPERATURE (1.5) - More creative and varied:\\n\")\n",
    "\n",
    "response = ollama.generate(\n",
    "    model='llama3.2',\n",
    "    prompt='Give me a name for a coffee shop.',\n",
    "    options={\n",
    "        'temperature': 1.5\n",
    "    }\n",
    ")\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Custom Configuration:\n",
      "\n",
      "Here are a few ideas:\n",
      "\n",
      "1. \"Empowering innovation, one byte at a time.\"\n",
      "2. \"Transforming the future, digitally born.\"\n",
      "3. \"Code, create, disrupt: The tech revolution begins now.\"\n",
      "4. \"Unlocking\n"
     ]
    }
   ],
   "source": [
    "# Combining multiple options\n",
    "print(\"âš™ï¸ Custom Configuration:\\n\")\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': 'Write a creative tagline for a tech startup.'\n",
    "    }],\n",
    "    options={\n",
    "        'temperature': 1.0,\n",
    "        'top_p': 0.9,\n",
    "        'top_k': 50,\n",
    "        'num_predict': 50\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ” Part 10: Listing Available Models\n",
    "\n",
    "You can programmatically list all models available on your system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Installed Models:\n",
      "============================================================\n",
      "\n",
      "ğŸ” Debug - First model structure:\n",
      "   Available attributes: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setitem__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_copy_and_set_values', '_get_value', '_iter', '_setattr_handler', 'construct', 'copy', 'details', 'dict', 'digest', 'from_orm', 'get', 'json', 'model', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'modified_at', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'size', 'update_forward_refs', 'validate']\n",
      "   Model object: model='llama3.2:latest' modified_at=datetime.datetime(2026, 1, 6, 13, 0, 5, 333852, tzinfo=TzInfo(19800)) digest='a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72' size=2019393189 details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.2B', quantization_level='Q4_K_M')\n",
      "\n",
      "  ğŸ“¦ llama3.2:latest\n",
      "     Size: 1.88 GB\n",
      "     Modified: 2026-01-06 13:00:05\n",
      "\n",
      "  ğŸ“¦ gemma3:1b\n",
      "     Size: 0.76 GB\n",
      "     Modified: 2026-01-06 12:28:27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all installed models\n",
    "models = ollama.list()\n",
    "\n",
    "print(\"ğŸ“‹ Installed Models:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# First, let's see the actual structure\n",
    "print(\"\\nğŸ” Debug - First model structure:\")\n",
    "if models.models:\n",
    "    first_model = models.models[0]\n",
    "    print(f\"   Available attributes: {dir(first_model)}\")\n",
    "    print(f\"   Model object: {first_model}\")\n",
    "print()\n",
    "\n",
    "# Now iterate with correct attribute access\n",
    "for model in models.models:\n",
    "    # Use attribute access (dot notation) instead of dict access\n",
    "    name = model.model  # 'model' attribute contains the name\n",
    "    size = getattr(model, 'size', 0) / (1024**3)  # Convert to GB\n",
    "    modified = getattr(model, 'modified_at', 'Unknown')\n",
    "    \n",
    "    print(f\"  ğŸ“¦ {name}\")\n",
    "    print(f\"     Size: {size:.2f} GB\")\n",
    "    print(f\"     Modified: {str(modified)[:19]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Part 11: Practical Examples\n",
    "\n",
    "Let's explore some practical use cases for Ollama in Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Code Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**What is the Fibonacci sequence?**\n",
       "\n",
       "The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones, usually starting with 0 and 1. The sequence looks like this:\n",
       "\n",
       "0, 1, 1, 2, 3, 5, 8, 13, ...\n",
       "\n",
       "**How does the Python code work?**\n",
       "\n",
       "The code defines a function called `fibonacci` that takes one argument, `n`. This function calculates the nth number in the Fibonacci sequence.\n",
       "\n",
       "Here's what happens when you call the function:\n",
       "\n",
       "1. **Base case**: If `n` is less than or equal to 1, the function returns `n`. This is because the first two numbers in the Fibonacci sequence are 0 and 1.\n",
       "2. **Recursive step**: If `n` is greater than 1, the function calls itself twice:\n",
       "\t* Once with `n-1` (the second-to-last number in the sequence)\n",
       "\t* And once with `n-2` (the last number in the sequence)\n",
       "3. The function returns the sum of these two recursive calls, which is the nth number in the Fibonacci sequence.\n",
       "\n",
       "**Example usage:**\n",
       "\n",
       "To calculate the 5th number in the Fibonacci sequence, you would call the function like this:\n",
       "\n",
       "```python\n",
       "print(fibonacci(5))  # Output: 5\n",
       "```\n",
       "\n",
       "However, please note that this recursive implementation can be slow and inefficient for large values of `n`, because it does a lot of repeated computation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "code_to_explain = \"\"\"\n",
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': f\"Explain this Python code in simple terms:\\n```python\\n{code_to_explain}\\n```\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "display(Markdown(response['message']['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Document Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Summary:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here are 3 bullet points summarizing the text:\n",
       "\n",
       "â€¢ Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed.\n",
       "â€¢ There are three main types of machine learning algorithms: supervised learning (using labeled data), unsupervised learning (discovering patterns in unlabeled data), and reinforcement learning (teaching agents to make decisions based on rewards).\n",
       "â€¢ Machine learning powers many daily applications, including recommendation systems and voice assistants."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "long_text = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence that focuses on enabling \n",
    "computers to learn from data without being explicitly programmed. The field has \n",
    "evolved significantly since its inception in the 1950s, with major breakthroughs \n",
    "in deep learning occurring in the 2010s. Machine learning algorithms can be broadly \n",
    "categorized into supervised learning, unsupervised learning, and reinforcement learning. \n",
    "Supervised learning uses labeled data to train models, while unsupervised learning \n",
    "discovers patterns in unlabeled data. Reinforcement learning teaches agents to make \n",
    "decisions by rewarding desired behaviors. Today, machine learning powers many \n",
    "applications we use daily, from recommendation systems to voice assistants.\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': f\"Summarize this text in 2-3 bullet points:\\n\\n{long_text}\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(\"ğŸ“ Summary:\")\n",
    "display(Markdown(response['message']['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Creative Writing Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Original Text:\n",
      "The sun was very hot. The man walked slowly. He was tired.\n",
      "\n",
      "âœ¨ Improved Version:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's a revised version of the text:\n",
       "\n",
       "\"The blistering sun beat down on the worn streets, its intense heat like a physical weight pressing against his skin. The man trudged forward, his footsteps slow and labored as if weighed down by an invisible burden. His eyes drooped with exhaustion, heavy-lidded from a long day's toil.\"\n",
       "\n",
       "I made several changes to enhance the text:\n",
       "\n",
       "* Added sensory details: I incorporated more descriptive language to help the reader experience the scene. For example, \"the blistering sun\" evokes a sense of heat and intensity.\n",
       "* Varied sentence structure: To create a more dynamic rhythm, I mixed short and longer sentences. This allows the reader's eye to move easily through the text.\n",
       "* Showed, rather than told: Instead of saying the man was tired, I showed his exhaustion through his physical behavior (drooping eyes) and emotional state (feeling weighed down).\n",
       "* Added more descriptive language: Phrases like \"invisible burden\" create a vivid image in the reader's mind and help to build tension.\n",
       "* Used active verbs: Verbs like \"trudged\" and \"beat down\" add more energy and movement to the text, making it feel more engaging.\n",
       "\n",
       "These changes aim to engage the reader by creating a richer, more immersive experience."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a creative writing assistant\n",
    "writer = OllamaChat(\n",
    "    model='llama3.2',\n",
    "    system_prompt=\"\"\"You are a creative writing assistant. Help users improve their writing \n",
    "    with suggestions for better word choices, structure, and style. Be encouraging but constructive.\"\"\"\n",
    ")\n",
    "\n",
    "user_text = \"The sun was very hot. The man walked slowly. He was tired.\"\n",
    "\n",
    "print(\"ğŸ“ Original Text:\")\n",
    "print(user_text)\n",
    "print(\"\\nâœ¨ Improved Version:\")\n",
    "\n",
    "response = writer.chat(f\"Please improve this text and make it more engaging: {user_text}\")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Python Helper / Debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› Bug Analysis:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The bug in the given code is that it does not handle the case when the input list `numbers` is empty. In this case, `len(numbers)` would be 0 and division by zero would result in a ZeroDivisionError.\n",
       "\n",
       "Here's how you can fix the issue:\n",
       "\n",
       "```python\n",
       "def calculate_average(numbers):\n",
       "    \"\"\"\n",
       "    Calculate the average of a list of numbers.\n",
       "\n",
       "    Args:\n",
       "        numbers (list): A list of numbers.\n",
       "\n",
       "    Returns:\n",
       "        float: The average of the input numbers.\n",
       "    \"\"\"\n",
       "    if not numbers:  # Check if the list is empty\n",
       "        raise ValueError(\"Cannot calculate average of an empty list\")\n",
       "\n",
       "    total = 0\n",
       "    for num in numbers:\n",
       "        total += num\n",
       "\n",
       "    return total / len(numbers)\n",
       "\n",
       "# Test with an empty list\n",
       "try:\n",
       "    result = calculate_average([])\n",
       "except ValueError as e:\n",
       "    print(e)  # Output: Cannot calculate average of an empty list\n",
       "```\n",
       "\n",
       "Alternatively, you can use the built-in `sum()` function to make the code more concise:\n",
       "\n",
       "```python\n",
       "def calculate_average(numbers):\n",
       "    \"\"\"\n",
       "    Calculate the average of a list of numbers.\n",
       "\n",
       "    Args:\n",
       "        numbers (list): A list of numbers.\n",
       "\n",
       "    Returns:\n",
       "        float: The average of the input numbers.\n",
       "    \"\"\"\n",
       "    if not numbers:\n",
       "        raise ValueError(\"Cannot calculate average of an empty list\")\n",
       "\n",
       "    return sum(numbers) / len(numbers)\n",
       "```\n",
       "\n",
       "Or, you can use a try-except block to catch and handle the ZeroDivisionError:\n",
       "\n",
       "```python\n",
       "def calculate_average(numbers):\n",
       "    \"\"\"\n",
       "    Calculate the average of a list of numbers.\n",
       "\n",
       "    Args:\n",
       "        numbers (list): A list of numbers.\n",
       "\n",
       "    Returns:\n",
       "        float: The average of the input numbers.\n",
       "    \"\"\"\n",
       "    try:\n",
       "        return sum(numbers) / len(numbers)\n",
       "    except ZeroDivisionError:\n",
       "        raise ValueError(\"Cannot calculate average of an empty list\")\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "buggy_code = \"\"\"\n",
    "def calculate_average(numbers):\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        total += num\n",
    "    return total / len(numbers)\n",
    "\n",
    "# This will crash!\n",
    "result = calculate_average([])\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': f\"Find the bug in this code and suggest a fix:\\n```python\\n{buggy_code}\\n```\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(\"ğŸ› Bug Analysis:\")\n",
    "display(Markdown(response['message']['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”„ Part 12: Using LangChain with Ollama (Alternative Approach)\n",
    "\n",
    "You can also use Ollama through **LangChain** for more advanced workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install LangChain with Ollama integration\n",
    "# !pip install langchain-ollama -q\n",
    "\n",
    "# print(\"âœ… LangChain-Ollama installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is a deeply personal and subjective question that has been explored by philosophers, theologians, scientists, and countless individuals throughout history; however, at its core, it often revolves around finding purpose, fulfillment, happiness, and connection with oneself, others, and the world.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Initialize Ollama through LangChain\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Simple invoke\n",
    "response = llm.invoke(\"What is the meaning of life in one sentence?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒŠ Streaming with LangChain:\n",
      "\n",
      "Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs.\n"
     ]
    }
   ],
   "source": [
    "# Streaming with LangChain\n",
    "print(\"ğŸŒŠ Streaming with LangChain:\\n\")\n",
    "\n",
    "for chunk in llm.stream(\"Tell me a joke about programming.\"):\n",
    "    print(chunk, end='', flush=True)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š Part 13: Performance Tips\n",
    "\n",
    "Here are some tips to get the best performance from Ollama:\n",
    "\n",
    "### 1. Choose the Right Model Size\n",
    "- **Limited RAM (8GB)**: Use `llama3.2:1b` or `phi3`\n",
    "- **Medium RAM (16GB)**: Use `llama3.2` or `mistral`\n",
    "- **High RAM (32GB+)**: Can handle larger models like `llama3.1:8b`\n",
    "\n",
    "### 2. GPU Acceleration\n",
    "Ollama automatically uses GPU if available. Check GPU usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if NVIDIA GPU is available (Windows/Linux)\n",
    "# !nvidia-smi --query-gpu=name,memory.used,memory.total --format=csv 2>nul || echo \"No NVIDIA GPU detected or nvidia-smi not available\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Monitoring Response Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Performance Metrics:\n",
      "   â±ï¸  Total Time: 16.24 seconds\n",
      "   ğŸ“ Tokens Generated: 95\n",
      "   ğŸš€ Speed: 5.9 tokens/second\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_response(prompt, model='llama3.2'):\n",
    "    \"\"\"Measure response time and tokens per second\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    duration = end_time - start_time\n",
    "    tokens_generated = response.get('eval_count', 0)\n",
    "    tokens_per_second = tokens_generated / duration if duration > 0 else 0\n",
    "    \n",
    "    print(f\"ğŸ“Š Performance Metrics:\")\n",
    "    print(f\"   â±ï¸  Total Time: {duration:.2f} seconds\")\n",
    "    print(f\"   ğŸ“ Tokens Generated: {tokens_generated}\")\n",
    "    print(f\"   ğŸš€ Speed: {tokens_per_second:.1f} tokens/second\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Run benchmark\n",
    "response = benchmark_response(\"Explain what an API is in 3 sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ Part 14: Bonus - Error Handling\n",
    "\n",
    "Always handle potential errors when working with LLMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid model test:\n",
      "Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "\n",
      "Invalid model test:\n",
      "âŒ Ollama Error: model 'nonexistent-model' not found (status code: 404)\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from ollama import ResponseError\n",
    "\n",
    "def safe_query(prompt, model='llama3.2'):\n",
    "    \"\"\"Safely query Ollama with error handling\"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{'role': 'user', 'content': prompt}]\n",
    "        )\n",
    "        return response['message']['content']\n",
    "    \n",
    "    except ResponseError as e:\n",
    "        if \"model not found\" in str(e).lower():\n",
    "            return f\"âŒ Error: Model '{model}' not found. Try: ollama pull {model}\"\n",
    "        return f\"âŒ Ollama Error: {e}\"\n",
    "    \n",
    "    except ConnectionError:\n",
    "        return \"âŒ Connection Error: Is Ollama running? Try: ollama serve\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"âŒ Unexpected error: {e}\"\n",
    "\n",
    "# Test with valid and invalid models\n",
    "print(\"Valid model test:\")\n",
    "print(safe_query(\"Say hello!\", model='llama3.2'))\n",
    "\n",
    "print(\"\\nInvalid model test:\")\n",
    "print(safe_query(\"Say hello!\", model='nonexistent-model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Summary\n",
    "\n",
    "In this tutorial, we covered:\n",
    "\n",
    "| Topic | Key Points |\n",
    "|-------|------------|\n",
    "| **Installation** | Ollama is easy to install on Windows, macOS, and Linux |\n",
    "| **Model Management** | Use `ollama pull` to download and `ollama list` to view models |\n",
    "| **Basic Queries** | Use `ollama.chat()` for conversations, `ollama.generate()` for simple prompts |\n",
    "| **Markdown Display** | Use `IPython.display.Markdown` or `rich` for beautiful output |\n",
    "| **Streaming** | Set `stream=True` for real-time token generation |\n",
    "| **Chat Applications** | Maintain message history for contextual conversations |\n",
    "| **Advanced Options** | Customize behavior with temperature, top_p, top_k |\n",
    "| **LangChain Integration** | Use `langchain-ollama` for advanced workflows |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Useful Resources\n",
    "\n",
    "- ğŸ“– [Ollama Official Website](https://ollama.com)\n",
    "- ğŸ“¦ [Ollama Python Library](https://github.com/ollama/ollama-python)\n",
    "- ğŸ“š [Ollama Model Library](https://ollama.com/library)\n",
    "- ğŸ”§ [LangChain Ollama Integration](https://python.langchain.com/docs/integrations/llms/ollama)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‰ Next Steps\n",
    "\n",
    "Now that you know how to use Ollama with Jupyter Notebook, try:\n",
    "\n",
    "1. ğŸ§ª Experiment with different models (Mistral, Phi, Gemma)\n",
    "2. ğŸ”§ Build a custom chatbot for your specific use case\n",
    "3. ğŸ“Š Create a data analysis assistant\n",
    "4. ğŸ–¼ï¸ Try vision models with `llama3.2-vision`\n",
    "5. ğŸ”— Integrate with RAG (Retrieval-Augmented Generation) pipelines\n",
    "\n",
    "Happy coding! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
